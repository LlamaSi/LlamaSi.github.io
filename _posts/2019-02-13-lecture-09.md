--- 
layout: distill
title: Lecture Notes Template
description: An example of a distill-style lecture notes that showcases the main elements.
date: 2019-01-09

lecturers:
  - name: Eric Xing
    url: "https://www.cs.cmu.edu/~epxing/"

authors:
  - name: Wenwen Si # author's full name
    url: "#"  # optional URL to the author's homepage
  - name: Author 2
    url: "#"
  - name: Author 3
    url: "#"

editors:
  - name: Editor 1  # editor's full name
    url: "#"  # optional URL to the editor's homepage

abstract: >
  An example abstract block.
  
---

# Lecture 09 Modeling Networks

## Network research - study at graph as object

Do graphs and networks really exsit in real world? No.
Where fo networks come from? Different rule for define the graph will generate different graph.
none of the network exsits physically

## Structural Learning
### Trees: The Chow-Liu algorithm
Directly search for optimal tree structure
### Pairwise Markov Random Fields
<figure>
  <img src="{{ '/assets/img/notes/lecture-09/node4network.png' | relative_url }}" />
</figure>
- Key idea:  network inference as parameter estimation
- every node: observations can include binary / continuous number
- vairables connected pairwise (pairwise MRF, also BM)

$$ p(x_1, x_2, x_3, x_4) = \frac{1}{Z} exp\{\theta_1x_1 + \theta_2x_2 + \theta_3x_3 + \theta_4x_4  +  \theta_{12}x_1x_2 + \theta_{13}x_1x_3 + \theta_{23}x_2x_3 + \theta_{34}x_3x_4 \} $$

- use non-zero parameters to represent edges, turn topology into continuous space
<figure>
  <img src="{{ '/assets/img/notes/lecture-09/matrix2topo.png' | relative_url }}" />
</figure>

- Model
  - discrete nodal states (Ising/Potts model)
  - continuous (Gaussian grahical model)
  - heterogeneous
- parameter matrix encodes graph structure (non zero $\iff$ edge)

#### Multivariant Gaussian 

  $$p(\vec x \mid \mu, \Sigma) = \frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}}exp\{-\frac{1}{2}(\vec x - \mu)^T \Sigma^{-1} (\vec x - \mu) \} $$
  
  - A continuous Markov Random Field with potentials defined on every node and edge
  - covariance matrix -inverse-> precision matrix (encodes dependencies )
  - Markov vs Correlation Network
    Markov fits more on real world because it can model conditioncal probabilities
    - Correlation network isbased on Covariance Matrix
    
    $$\Sigma_{i,j} = 0 \Rightarrow X_i \perp X_j \quad or \quad p(X_i, X_j) = p(X_i)p(X_j)$$
    
    - A GGM is a Markov Networkbased on Precision Matrix
      - Conditional Independence/Partial Correlation Coefficients are a more sophisticated dependence measure
      
      $$Q_{i,j}=0 \Rightarrow X_i \perp X_j \mid X_{-ij} \quad or  \quad p(X_i, X_j \mid X_{-ij}) = p(X_i \mid X_{-ij})p(X_j \perp X_{-ij})$$
      
<figure>
  <img src="{{ '/assets/img/notes/lecture-09/gaussian.png' | relative_url }}" />
</figure>

  - Problem
    - you want the precison matrix to be invertible, but non full rank does not mean MN does not exist
    - With small sample size, empirical convariance matrix cannot be inverted
    - sparsity: inversion of matrix not scalable
## Network Learning
### learning with LASSO
- perform lasso regression
$$x_1 = \vec \beta \vec x_{-1} + const$$
  - problems: we can not get 0 in $$\beta$$
  - solution: put constraint \highlight{equation missing}
    - L1 Regression (LASSO):
- recursively search each node \Rightarrow a whole graph
  - each node traveled twice: add a rule \highlight{missing equation}
  $$ \mathcal E = \{(u, v): max(|\hat \beta_{uv}|, |\hat \beta_{vu}|) > 0\}
### consistent Structure Recovery
## Why these algorithm work?
### multivariate Gaussian
- Properties
  - $$P(\vec x \mid \mu \Sigma)$$
  - $$ p(x_2), p(x_1 \mid x_2) \quad p(x_2 \mid x_1) $$
   marginal and conditional probability are all Gaussian with formulas to remember
- Single-node Conditional
  - single node i given the rest of the nodes analytical solution
  $$p(X_i \ mid X_{-i}) $$
  - conditional auto-regression
    - $$x_i = \theta \vec x_{-i} + \sigma \quad \sigma \sim \mathcal N(0, \sigma)
### Ising Model
- discrete node
  - it can be shown the pseudo-conditional likelihood for node k is 
    $$P_\theta (x_k \mid x_{-k}) = logitic(2x <\theta_{-k}, x_{-k}>)$$