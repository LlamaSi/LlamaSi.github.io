# Lecture 08 Modeling Networks

## Network research - study at graph as object

Do graphs and networks really exsit in real world? No.
Where fo networks come from? Different rule for define the graph will generate different graph.
none of the network exsits physically

## Structural Learning
### Trees: The Chow-Liu algorithm
### Pairwise Markov Random Fields
- directly search for optimal tree structure
- every node: observations can include binary / continuous number
- vairables connected pairwise (pairwise MRF, also BM)
$$ p(x_1, x_2, x_3, x_4) = \frac{1}{Z} exp\{\theta_1x_1 + \theta_2x_2 + \theta_3x_3 + \theta_4x_4  +  \theta_{12}x_1x_2 + \theta_{13}x_1x_3 + \theta_{23}x_2x_3 + \theta_{34}x_3x_4 \}$$

- use non-zero parameters to represent edges, turn topology into continuous space
#### Model
- discrete nodal states - Ising/Potts model 
  continuous - Gaussian grahical model 
  heterogeneous
- parameter matrix encodes graph structure (non zero \iff edge)
- Multivariant Gaussian 
  - covariance matrix -inverse-> precision matrix (encodes dependencies )
  - assume 
    $$ X^{(n)} \sim \mathcal N(0, \Sigma) $$
  - Markov vs Correlation Network
    Markov fitts more on real world because it can model conditioncal probabilities
    - you want the precison matrix to be invertible, but non full rank does not mean MN does not exist
    - inversion of matrix not scalable
## Network Learning
### learning with LASSO
- perform lasso regression
$$x_1 = \vec \beta \vec x_{-1} + const$$
  - problems: we can not get 0 in $$\beta$$
  - solution: put constraint \highlight{equation missing}
    - L1 Regression (LASSO):
- recursively search each node \Rightarrow a whole graph
  - each node traveled twice: add a rule \highlight{missing equation}
  $$ \mathcal E = \{(u, v): max(|\hat \beta_{uv}|, |\hat \beta_{vu}|) > 0\}
### consistent Structure Recovery
## Why these algorithm work?
### multivariate Gaussian
- Properties
  - $$P(\vec x \mid \mu \Sigma)$$
  - $$ p(x_2), p(x_1 \mid x_2) \quad p(x_2 \mid x_1) $$
   marginal and conditional probability are all Gaussian with formulas to remember
- Single-node Conditional
  - single node i given the rest of the nodes analytical solution
  $$p(X_i \ mid X_{-i}) $$
  - conditional auto-regression
    - $$x_i = \theta \vec x_{-i} + \sigma \quad \sigma \sim \mathcal N(0, \sigma)
### Ising Model
- discrete node
  - it can be shown the pseudo-conditional likelihood for node k is 
    $$P_\theta (x_k \mid x_{-k}) = logitic(2x <\theta_{-k}, x_{-k}>)$$